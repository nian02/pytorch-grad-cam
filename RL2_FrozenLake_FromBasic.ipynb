{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b94bdbe3df874115998ec0e0df2543fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f1c361dea5248e2a0daa94ae72694e4",
              "IPY_MODEL_d809bf409f98481b88984f3f50182470",
              "IPY_MODEL_205fedb73be7466a93a931d14d161399"
            ],
            "layout": "IPY_MODEL_d247aa3aff0143f2b49cd22450fa889e"
          }
        },
        "6f1c361dea5248e2a0daa94ae72694e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c89752b086443de8664a9954397ac3a",
            "placeholder": "​",
            "style": "IPY_MODEL_3826ae4e979b4892a6065f6ae981b677",
            "value": "100%"
          }
        },
        "d809bf409f98481b88984f3f50182470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53534c2c7d0e47959c496f00bde74b32",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f049d049643434287dde6fa23373e5d",
            "value": 10000
          }
        },
        "205fedb73be7466a93a931d14d161399": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_222adc55450c4d02aa3735b3e2b6466b",
            "placeholder": "​",
            "style": "IPY_MODEL_7c98a5c7edcc49d392425f4dce8ed4f0",
            "value": " 10000/10000 [00:01&lt;00:00, 7734.67it/s]"
          }
        },
        "d247aa3aff0143f2b49cd22450fa889e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c89752b086443de8664a9954397ac3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3826ae4e979b4892a6065f6ae981b677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53534c2c7d0e47959c496f00bde74b32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f049d049643434287dde6fa23373e5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "222adc55450c4d02aa3735b3e2b6466b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c98a5c7edcc49d392425f4dce8ed4f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ada78b632e84d87ae43b2f76473fcad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ff42d4fefd74a9cb3149fee5b765207",
              "IPY_MODEL_78bd099a19694296b7b658f68ae5b62b",
              "IPY_MODEL_4e70251f0ed444e0a043732d078a188e"
            ],
            "layout": "IPY_MODEL_2fa0154be64645d2ad7247341da41e92"
          }
        },
        "9ff42d4fefd74a9cb3149fee5b765207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7478c74676b04e38b1c3937a551cb50e",
            "placeholder": "​",
            "style": "IPY_MODEL_da5cde5eab684c1eb3d2cd762e811c95",
            "value": "100%"
          }
        },
        "78bd099a19694296b7b658f68ae5b62b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18db73cbc98a4454ac4be4b6f4ea7f38",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4f7d198b9204883bc006f8ee42487ae",
            "value": 100
          }
        },
        "4e70251f0ed444e0a043732d078a188e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51c31013e0f249c1879368a181f9cd2d",
            "placeholder": "​",
            "style": "IPY_MODEL_61cc4ed3e49349ae82ab7b9ceaab48e9",
            "value": " 100/100 [00:00&lt;00:00, 3619.93it/s]"
          }
        },
        "2fa0154be64645d2ad7247341da41e92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7478c74676b04e38b1c3937a551cb50e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da5cde5eab684c1eb3d2cd762e811c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18db73cbc98a4454ac4be4b6f4ea7f38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4f7d198b9204883bc006f8ee42487ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51c31013e0f249c1879368a181f9cd2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61cc4ed3e49349ae82ab7b9ceaab48e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nian02/pytorch-grad-cam/blob/master/RL2_FrozenLake_FromBasic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 의존성 설치 및 가상 디스플레이 생성 🔽\n",
        "\n",
        "노트북에서는 재생 비디오를 생성할 필요가 있습니다. 이를 위해 Colab을 사용할 때 **환경을 렌더링하기 위한 가상 화면이 필요**합니다(그리고 따라서 프레임을 기록합니다).\n",
        "\n",
        "따라서 다음 셀은 라이브러리를 설치하고 가상 화면을 생성 및 실행합니다 🖥\n",
        "\n",
        "여러 가지를 설치할 것입니다:\n",
        "\n",
        "- `gymnasium`: FrozenLake-v1 ⛄ 및 Taxi-v3 🚕 환경을 포함합니다.\n",
        "- `pygame`: FrozenLake-v1 및 Taxi-v3 UI에 사용됩니다.\n",
        "- `numpy`: Q-table을 다루는 데 사용됩니다.\n",
        "\n",
        "Hugging Face Hub 🤗는 누구나 모델과 데이터셋을 공유하고 탐색할 수 있는 중앙 장소로 기능합니다. 버전 관리, 메트릭, 시각화 및 다른 기능들이 협업을 쉽게 할 수 있도록 도와줍니다.\n",
        "\n",
        "여기에서 Q Learning을 사용하는 모든 Deep RL 모델을 볼 수 있습니다 👉 https://huggingface.co/models?other=q-learning"
      ],
      "metadata": {
        "id": "kbdOKmY10Ocg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7LrCM7Mz4vf",
        "outputId": "c2185552-c770-4995-f819-0fefe073196e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1))\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.9/953.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 2)) (2.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 3)) (1.25.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (0.20.3)\n",
            "Collecting pickle5 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 6))\n",
            "  Downloading pickle5-0.0.11.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.1/132.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyyaml==6.0 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 7))\n",
            "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 8)) (2.31.6)\n",
            "Requirement already satisfied: imageio_ffmpeg in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 9)) (0.4.9)\n",
            "Collecting pyglet==1.5.1 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 10))\n",
            "  Downloading pyglet-1.5.1-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 11)) (4.66.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1)) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1)) (4.10.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1))\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (2.31.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (24.0)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 8)) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio_ffmpeg->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 9)) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (2024.2.2)\n",
            "Building wheels for collected packages: pickle5\n",
            "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle5: filename=pickle5-0.0.11-cp310-cp310-linux_x86_64.whl size=255319 sha256=5e826e009404e73e41a43efc2f7b50fa1dde0403116f0003f1eabc77b4c60847\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/14/ef/4aab19d27fa8e58772be5c71c16add0426acf9e1f64353235c\n",
            "Successfully built pickle5\n",
            "Installing collected packages: pyglet, pickle5, farama-notifications, pyyaml, gymnasium\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 pickle5-0.0.11 pyglet-1.5.1 pyyaml-6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colab 환경 설정\n",
        "\n",
        "Colab에서는 일부 환경 설정이나 라이브러리 설치를 위해 Linux 패키지 관리자인 `apt`와 Python 패키지 관리자인 `pip` 명령어를 사용합니다. 아래 설명은 각 명령어가 수행하는 작업에 대해 설명합니다.\n",
        "\n",
        "### 시스템 패키지 업데이트\n",
        "\n",
        "```markdown\n",
        "!sudo apt-get update\n",
        "```\n",
        "이 명령어는 사용 가능한 모든 패키지의 목록을 최신 상태로 업데이트합니다. 이 과정을 통해 다음에 설치할 패키지들의 최신 버전을 가져올 수 있습니다.\n",
        "\n",
        "### Python OpenGL 설치\n",
        "\n",
        "```markdown\n",
        "!sudo apt-get install -y python3-opengl\n",
        "```\n",
        "이 명령어는 Python을 위한 OpenGL 라이브러리를 설치합니다. OpenGL은 크로스 플랫폼의 그래픽 API로, 3D 그래픽 프로그래밍에 주로 사용됩니다. 이 라이브러리는 Python 프로그램에서 3D 그래픽스를 렌더링하기 위해 필요합니다.\n",
        "\n",
        "### FFmpeg 및 Xvfb 설치\n",
        "\n",
        "```markdown\n",
        "!apt install ffmpeg xvfb\n",
        "```\n",
        "- `ffmpeg`: 동영상 파일의 변환, 스트리밍 및 재생을 위한 커맨드라인 도구입니다. 동영상 처리나 오디오/비디오 기록, 변환 작업에 사용됩니다.\n",
        "- `xvfb`: X Virtual FrameBuffer는 그래픽 디스플레이 기능을 제공하지 않는 서버에서 GUI 프로그램을 실행할 수 있게 해주는 가상 디스플레이 서버입니다. 즉, 실제 디스플레이 없이 그래픽 애플리케이션을 실행할 수 있게 해줍니다.\n",
        "\n",
        "### PyVirtualDisplay 설치\n",
        "\n",
        "```markdown\n",
        "!pip3 install pyvirtualdisplay\n",
        "```\n",
        "`PyVirtualDisplay`는 Python으로 가상 디스플레이를 쉽게 생성하고 관리할 수 있게 해주는 라이브러리입니다. `Xvfb`, `Xephyr`, `Xvnc`와 같은 가상 디스플레이 서버를 사용하여, 실제 모니터 없이도 GUI 프로그램을 실행하는 환경을 구성할 수 있습니다. 이는 특히 서버 환경이나 자동화된 테스트에서 유용하게 사용됩니다."
      ],
      "metadata": {
        "id": "iJFQzVmG0sp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!apt install ffmpeg xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1qRiCaEz8Hz",
        "outputId": "5bb76eb2-3965-4eeb-8901-b6e0968154f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.82)] [Co\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [2 InRelease 80.8 kB/119 kB 68%] [Connecting to security.ubuntu.com (91.189.\r                                                                               \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\r                                                                    \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\r                                                                    \rHit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "\r                                                                    \r0% [Waiting for headers] [Waiting for headers]\r                                              \rHit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "\r                                              \r0% [Waiting for headers]\r                        \rHit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers]\r                        \rHit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers]\r                                              \rHit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,920 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,358 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,104 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,641 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,081 kB]\n",
            "Fetched 8,337 kB in 2s (4,402 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  freeglut3 libglu1-mesa\n",
            "Suggested packages:\n",
            "  libgle3 python3-numpy\n",
            "The following NEW packages will be installed:\n",
            "  freeglut3 libglu1-mesa python3-opengl\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 824 kB of archives.\n",
            "After this operation, 8,092 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-opengl all 3.1.5+dfsg-1 [605 kB]\n",
            "Fetched 824 kB in 0s (7,489 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "(Reading database ... 121753 files and directories currently installed.)\n",
            "Preparing to unpack .../freeglut3_2.8.1-6_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-6) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package python3-opengl.\n",
            "Preparing to unpack .../python3-opengl_3.1.5+dfsg-1_all.deb ...\n",
            "Unpacking python3-opengl (3.1.5+dfsg-1) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-6) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up python3-opengl (3.1.5+dfsg-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 7,814 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.8 [28.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.8 [863 kB]\n",
            "Fetched 7,814 kB in 0s (48.1 MB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 124837 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.8_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.8) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.8_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.8) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.8) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.8) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "새로 설치된 라이브러리가 사용되도록 하기 위해서, **때로는 노트북 런타임을 재시작해야 하는 경우가 있습니다**. 다음 셀은 **런타임을 강제로 종료시키므로, 다시 연결한 후 여기서부터 코드를 실행해야 합니다**. 이 트릭 덕분에, **우리는 가상 화면을 실행할 수 있게 됩니다**."
      ],
      "metadata": {
        "id": "mFxno1ZV06i1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # os 모듈을 불러옵니다. os 모듈은 운영체제와 상호작용하는 다양한 기능을 제공합니다.\n",
        "\n",
        "# 현재 실행 중인 Python 스크립트(프로세스)를 강제 종료합니다.\n",
        "os.kill(os.getpid(), 9)  # os.getpid()는 현재 프로세스의 ID를 반환합니다. 숫자 9는 SIGKILL 신호를 나타냅니다."
      ],
      "metadata": {
        "id": "6vBUo-kS09iR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display  # pyvirtualdisplay 패키지로부터 Display 클래스를 임포트합니다.\n",
        "# 이 클래스는 가상 디스플레이를 생성하고 관리하는데 사용됩니다.\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))  # 가상 디스플레이 인스턴스를 생성합니다.\n",
        "# 'visible=0' 옵션은 디스플레이가 실제로 보이지 않게 설정합니다(즉, 백그라운드에서 실행).\n",
        "# 'size=(1400, 900)' 옵션은 가상 디스플레이의 해상도를 1400x900으로 설정합니다.\n",
        "\n",
        "virtual_display.start()  # 생성한 가상 디스플레이를 시작합니다.\n",
        "# 이 코드를 실행함으로써, 프로그램은 마치 물리적인 모니터가 존재하는 것처럼 GUI 애플리케이션을 렌더링할 수 있습니다.\n",
        "# 이는 서버나 헤드리스 환경에서 GUI 기반 애플리케이션을 실행할 때 유용합니다.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7FqB07X1MHI",
        "outputId": "cc6d90bf-1847-4429-a770-5dba5b139fbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7db1c728fc40>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 패키지 임포트 📦\n",
        "\n",
        "설치된 라이브러리 외에도 다음을 사용합니다:\n",
        "\n",
        "- `random`: 임의의 숫자를 생성하기 위해 사용됩니다(이는 엡실론-탐욕 정책에 유용합니다).\n",
        "- `imageio`: 재생 비디오를 생성하기 위해 사용됩니다."
      ],
      "metadata": {
        "id": "3H5x6Rly1XiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # 수학적 계산과 배열, 행렬 연산을 위한 라이브러리인 numpy를 임포트합니다.\n",
        "import gymnasium as gym  # 강화 학습 환경을 제공하는 gymnasium 라이브러리를 gym이라는 이름으로 임포트합니다.\n",
        "import random  # 난수 생성을 위한 라이브러리인 random을 임포트합니다.\n",
        "import imageio  # 이미지를 읽고 쓰는 작업을 위한 라이브러리인 imageio를 임포트합니다.\n",
        "import os  # 운영 체제와 상호작용하기 위한 다양한 기능을 제공하는 os 모듈을 임포트합니다.\n",
        "import tqdm  # 반복 작업의 진행 상황을 시각적으로 표시하기 위한 라이브러리인 tqdm을 임포트합니다.\n",
        "\n",
        "import pickle5 as pickle  # 객체를 파일로 저장하거나 파일에서 객체를 불러오기 위한 pickle 라이브러리를 pickle5 버전으로 임포트하고, pickle로 이름을 지정합니다.\n",
        "from tqdm.notebook import tqdm  # Jupyter 노트북에서 사용하기 적합하도록 tqdm 라이브러리의 notebook 모듈에서 tqdm을 임포트합니다."
      ],
      "metadata": {
        "id": "mjh5oxpv1FCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Frozen Lake ⛄ (non slippery version)"
      ],
      "metadata": {
        "id": "IHfWcRE_1too"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FrozenLake 환경 생성 및 이해하기 ⛄\n",
        "\n",
        "---\n",
        "\n",
        "💡 환경을 사용하기 시작할 때 좋은 습관은 그 환경의 문서를 확인하는 것입니다.\n",
        "\n",
        "👉 https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
        "\n",
        "---\n",
        "\n",
        "우리는 Q-Learning 에이전트를 훈련시켜 **시작 상태(S)에서 목표 상태(G)로 오직 얼어붙은 타일(F)만 걸어서 이동하고 구멍(H)을 피하도록** 할 것입니다.\n",
        "\n",
        "환경에는 두 가지 크기가 있습니다:\n",
        "\n",
        "- `map_name=\"4x4\"`: 4x4 그리드 버전\n",
        "- `map_name=\"8x8\"`: 8x8 그리드 버전\n",
        "\n",
        "환경에는 두 가지 모드가 있습니다:\n",
        "\n",
        "- `is_slippery=False`: 얼어붙은 호수의 미끄럽지 않은 특성 때문에 에이전트는 항상 **의도한 방향으로 이동**합니다 (결정론적).\n",
        "- `is_slippery=True`: 얼어붙은 호수의 미끄러운 특성 때문에 에이전트가 **항상 의도한 방향으로 이동하지는 않을 수 있습니다** (확률적)."
      ],
      "metadata": {
        "id": "U0Mw6TFK2CKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "일단 간단하게 4x4 맵과 미끄럽지 않은 환경으로 시작해보겠습니다.\n",
        "`render_mode`라고 하는 파라미터를 추가하는데, 이는 환경이 어떻게 시각화되어야 하는지를 명시합니다. 우리 경우에는 **마지막에 환경의 비디오를 녹화하고 싶기 때문에, render_mode를 rgb_array로 설정해야 합니다**.\n",
        "\n",
        "[문서에서 설명한 바와 같이](https://gymnasium.farama.org/api/env/#gymnasium.Env.render) “rgb_array”: 환경의 현재 상태를 나타내는 단일 프레임을 반환합니다. 프레임은 (x, y, 3) 모양의 np.ndarray로, x-by-y 픽셀 이미지에 대한 RGB 값을 나타냅니다."
      ],
      "metadata": {
        "id": "mdSG2cx_6ry2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the FrozenLake-v1 environment using 4x4 map and non-slippery version and render_mode=\"rgb_array\"\n",
        "env = gym.make() # TODO use the correct parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "im_artJy6vKS",
        "outputId": "00fe727a-c215-4400-9371-40d9d7d1b95a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "make() missing 1 required positional argument: 'id'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c616fa5f0142>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create the FrozenLake-v1 environment using 4x4 map and non-slippery version and render_mode=\"rgb_array\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO use the correct parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: make() missing 1 required positional argument: 'id'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "l0TXDtlw63tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gym 라이브러리를 사용하여 \"FrozenLake-v1\" 환경을 생성합니다. 이 때, 몇 가지 파라미터를 설정하여 환경을 커스터마이즈합니다.\n",
        "env = gym.make(\n",
        "    \"FrozenLake-v1\",  # 환경의 이름. 여기서는 \"FrozenLake-v1\"이라는 버전의 Frozen Lake 게임 환경을 사용합니다.\n",
        "    map_name=\"4x4\",  # 환경의 맵 크기를 지정합니다. \"4x4\"는 4x4 격자의 작은 맵을 의미합니다.\n",
        "    is_slippery=False,  # 미끄러짐 여부를 설정합니다. False는 미끄럽지 않은(결정론적인 이동이 가능한) 환경을 의미합니다.\n",
        "    render_mode=\"rgb_array\"  # 환경의 시각화 방식을 설정합니다. \"rgb_array\"는 환경의 현재 상태를 나타내는 RGB 배열을 반환하도록 설정합니다.\n",
        "    # 이는 나중에 비디오 녹화나 시각화에 유용하게 사용될 수 있습니다.\n",
        ")\n"
      ],
      "metadata": {
        "id": "VjkJ8E4d65dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이렇게 자신만의 맞춤형 격자를 생성할 수 있습니다:\n",
        "\n",
        "```python\n",
        "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
        "gym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n",
        "```\n",
        "\n",
        "하지만 우리는 지금은 기본 환경을 사용할 것입니다."
      ],
      "metadata": {
        "id": "fc-zvdgz6-m7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's see what the Environment looks like:\n"
      ],
      "metadata": {
        "id": "f1vhlBED7Oru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 우리는 gym.make(\"<환경의_이름>\")을 사용하여 환경을 생성합니다. - `is_slippery=False`: 얼어붙은 호수의 미끄럽지 않은 특성 때문에 에이전트는 항상 의도한 방향으로 이동합니다 (결정론적).\n",
        "print(\"_____관측 공간_____ \\n\")\n",
        "print(\"관측 공간\", env.observation_space)\n",
        "print(\"샘플 관측치\", env.observation_space.sample()) # 임의의 관측치를 얻습니다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQCGhPgH7DdD",
        "outputId": "076a4f2d-ca39-4fa2-e831-480aec284e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_____관측 공간_____ \n",
            "\n",
            "관측 공간 Discrete(16)\n",
            "샘플 관측치 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Observation Space Shape Discrete(16)`을 보면, 관측치는 **에이전트의 현재 위치를 현재 행 * 열의 수 + 현재 열 (여기서 행과 열 모두 0에서 시작)**로 나타내는 정수입니다.\n",
        "\n",
        "예를 들어, 4x4 맵에서 목표 위치는 다음과 같이 계산될 수 있습니다: 3 * 4 + 3 = 15. 가능한 관측치의 수는 맵의 크기에 따라 다릅니다. **예를 들어, 4x4 맵은 16개의 가능한 관측치가 있습니다.**\n",
        "\n",
        "예를 들어, 이것이 state = 0일 때의 모습입니다:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png\" alt=\"FrozenLake\">"
      ],
      "metadata": {
        "id": "mXv0SH7b7bLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n _____행동 공간_____ \\n\")\n",
        "print(\"행동 공간 크기\", env.action_space.n)\n",
        "print(\"행동 공간 샘플\", env.action_space.sample()) # 임의의 행동을 취합니다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCSN53zF7BMW",
        "outputId": "eb37ce4c-9541-4d1b-835a-38edca914b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " _____행동 공간_____ \n",
            "\n",
            "행동 공간 크기 4\n",
            "행동 공간 샘플 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "에이전트가 취할 수 있는 가능한 행동의 집합인 행동 공간은 4가지 행동이 가능한 이산적 공간입니다 🎮:\n",
        "- 0: 왼쪽으로 이동\n",
        "- 1: 아래로 이동\n",
        "- 2: 오른쪽으로 이동\n",
        "- 3: 위로 이동\n",
        "\n",
        "보상 함수 💰:\n",
        "- 목표 도달: +1\n",
        "- 구멍 도달: 0\n",
        "- 얼음 도달: 0"
      ],
      "metadata": {
        "id": "SRsl0wWr7ntv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-테이블 생성 및 초기화 🗄️\n",
        "\n",
        "(👀 의사 코드의 1단계)\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
        "\n",
        "Q-테이블을 초기화할 시간입니다! 몇 개의 행(상태)과 열(행동)을 사용해야 하는지 알기 위해서는, 행동과 관측 공간을 알아야 합니다. 이전에 그 값들을 알고 있지만, 다양한 환경에 대해 우리 알고리즘이 일반화되도록 프로그래밍적으로 그것들을 얻고 싶을 것입니다. Gym은 이를 위한 방법을 제공합니다: `env.action_space.n` 과 `env.observation_space.n`"
      ],
      "metadata": {
        "id": "dUfBt4QB7sJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_space =\n",
        "print(\"There are \", state_space, \" possible states\")\n",
        "\n",
        "action_space =\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ],
      "metadata": {
        "id": "4UiZpnNn7z0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "c20a8be1-0d4b-4d52-adc3-13e1511ccc94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-14-35328d4257a5>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-35328d4257a5>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    state_space =\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros. np.zeros needs a tuple (a,b)\n",
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable =\n",
        "  return Qtable"
      ],
      "metadata": {
        "id": "39qAG7LK7q8R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "e6456734-ec56-4e05-b981-767b00fe0376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-15-5aeba7260327>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-5aeba7260327>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    Qtable =\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
      ],
      "metadata": {
        "id": "QBfAQK3b76Qe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "faeba159-ce7f-41ee-e000-ff4d6ff2a33d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'initialize_q_table' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-75354c0f9143>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQtable_frozenlake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_q_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'initialize_q_table' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "dypbjhL_77FV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_space = env.observation_space.n\n",
        "print(\"There are \", state_space, \" possible states\")\n",
        "\n",
        "action_space = env.action_space.n\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q0bL4IT7882",
        "outputId": "8ef3a1af-2136-46e5-e563-224cb6016ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are  16  possible states\n",
            "There are  4  possible actions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n",
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable = np.zeros((state_space, action_space))\n",
        "  return Qtable"
      ],
      "metadata": {
        "id": "P24cLRr-7-Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
      ],
      "metadata": {
        "id": "Z04jUudu8Aa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 탐욕 정책 정의 🤖\n",
        "\n",
        "Q-Learning이 **off-policy** 알고리즘인 것을 기억합시다. 이는 우리가 **행동과 가치 함수의 업데이트를 위해 다른 정책을 사용**한다는 것을 의미합니다.\n",
        "\n",
        "- Epsilon-greedy 정책 (행동 정책)\n",
        "- Greedy 정책 (업데이트 정책)\n",
        "\n",
        "Greedy 정책은 Q-learning 에이전트가 훈련을 완료했을 때 우리가 가지게 될 최종 정책이기도 합니다. Greedy 정책은 Q-table을 사용하여 행동을 선택하는 데 사용됩니다.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>"
      ],
      "metadata": {
        "id": "YgBlMjtH8Ome"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "탐욕 정책(Greedy Policy)과 엡실론 탐욕 정책(Epsilon-Greedy Policy)은 강화 학습, 특히 Q-Learning과 같은 가치 기반 방법에서 에이전트의 행동 선택 방법을 결정하는 데 사용되는 중요한 전략입니다.\n",
        "\n",
        "### 탐욕 정책 (Greedy Policy)\n",
        "\n",
        "탐욕 정책은 가장 간단한 형태의 행동 선택 정책 중 하나입니다. 이 정책에 따르면, 에이전트는 현재 상태에서 가능한 모든 행동에 대해 예상되는 가치가 가장 높은 행동을 선택합니다. 즉, 가장 높은 Q-값을 가진 행동을 선택하는 것입니다. 이 접근 방식은 매우 효율적이지만, 새로운 환경을 탐색하는 데 있어서는 한계가 있습니다. 에이전트가 충분히 탐색하지 않고 빠르게 최적의 경로를 찾아내려고 하면, 최적이 아닌 솔루션에 수렴할 위험이 있습니다.\n",
        "\n",
        "### 엡실론 탐욕 정책 (Epsilon-Greedy Policy)\n",
        "\n",
        "엡실론 탐욕 정책은 탐욕 정책의 한계를 극복하기 위해 탐색(Exploration)과 활용(Exploitation) 사이의 균형을 맞추는 전략입니다. 이 정책은 대부분의 시간 동안은 최적의 행동(가장 높은 Q-값을 가진 행동)을 선택하여 활용하지만, 정해진 확률 엡실론(ε)에 따라 무작위 행동을 선택하여 탐색합니다. 이 확률은 일반적으로 0과 1 사이의 작은 값으로 설정됩니다.\n",
        "\n",
        "엡실론의 값에 따라, 에이전트는 새로운 행동을 탐색할 기회를 가지게 되며, 이는 장기적으로 더 나은 결정을 내릴 수 있도록 합니다. 예를 들어, ε = 0.1이라면, 에이전트는 90%의 확률로 최적의 행동을 선택하고, 10%의 확률로 임의의 행동을 선택합니다.\n",
        "\n",
        "### 정책의 선택\n",
        "\n",
        "- **탐욕 정책**은 확실한 환경에서 빠르게 최적의 해답을 찾을 때 유용합니다.\n",
        "- **엡실론 탐욕 정책**은 탐색이 중요한 불확실한 환경에서 더 나은 선택입니다. 이 정책을 통해 에이전트는 더 많은 상태와 행동을 경험하며, 최적의 정책을 발견할 가능성이 높아집니다.\n",
        "\n",
        "엡실론 탐욕 정책은 강화 학습에서 흔히 사용되는 전략이며, Q-Learning이나 SARSA와 같은 알고리즘에서 에이전트가 다양한 상황을 탐색하고 학습하는 데 필수적인 역할을 합니다."
      ],
      "metadata": {
        "id": "buis5gOH8jRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_policy(Qtable, state):\n",
        "  # Exploitation: take the action with the highest state, action value\n",
        "  action =\n",
        "\n",
        "  return action"
      ],
      "metadata": {
        "id": "NA_e7iFk8jmv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "40b09508-1d31-4473-b430-425632791293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-20-465ed89b6eee>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-465ed89b6eee>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    action =\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "cskGa55h8ozG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_policy(Qtable, state):\n",
        "  # Exploitation: take the action with the highest state, action value\n",
        "  action = np.argmax(Qtable[state][:])\n",
        "\n",
        "  return action"
      ],
      "metadata": {
        "id": "vU6HCM8n8pJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "본 문제에서 제시된 탐욕 정책(Greedy Policy) 함수는 에이전트가 현재 상태에서 가능한 행동들 중에서 예상되는 가치(Q-값)가 가장 높은 행동을 선택하도록 설계되어 있습니다. 이 함수는 Q-테이블과 현재 상태를 입력으로 받아, 해당 상태에서 가장 높은 Q-값을 가진 행동을 선택합니다. 이는 \"활용(Exploitation)\"의 개념에 기반한 것으로, 에이전트는 이미 알고 있는 정보를 기반으로 최적의 보상을 얻기 위한 행동을 선택합니다.\n",
        "\n",
        "함수의 구현은 다음과 같은 과정을 따릅니다:\n",
        "- `Qtable[state][:]`: 현재 상태에 해당하는 Q-테이블의 행을 가져옵니다. 이 행은 해당 상태에서 각 행동을 선택했을 때의 예상되는 가치(Q-값)를 포함하고 있습니다.\n",
        "- `np.argmax(Qtable[state][:])`: numpy의 `argmax` 함수를 사용하여 가장 높은 Q-값을 가진 행동의 인덱스(즉, 선택해야 할 행동)를 찾습니다. 이는 현재 상태에서 에이전트가 취할 수 있는 최적의 행동을 나타냅니다.\n",
        "\n",
        "이러한 탐욕 정책은 간단하고 직관적이지만, 항상 새로운 환경을 탐색하지는 않기 때문에 에이전트가 지역 최적해에 갇힐 위험이 있습니다. 즉, 에이전트가 다양한 경험을 충분히 하지 못하고 알려진 경로만을 반복하게 되어, 전체적인 최적해를 찾지 못할 수 있습니다. 이를 보완하기 위해 엡실론 탐욕 정책(Epsilon-Greedy Policy)과 같은 전략이 사용되어, 일정 확률로 무작위 행동을 선택하게 하여 탐색(Exploration)과 활용(Exploitation) 사이의 균형을 맞출 수 있습니다."
      ],
      "metadata": {
        "id": "6jdEa1Wu9S9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 엡실론 탐욕 정책 정의 🤖\n",
        "\n",
        "엡실론 탐욕(Epsilon-greedy)은 탐색과 활용 사이의 균형을 다루는 훈련 정책입니다.\n",
        "\n",
        "엡실론 탐욕의 아이디어:\n",
        "\n",
        "- *확률 1 - ε*로: **활용을 합니다** (즉, 우리의 에이전트는 가장 높은 상태-행동 쌍 가치를 가진 행동을 선택합니다).\n",
        "\n",
        "- *확률 ε*로: **탐색을 합니다** (임의의 행동을 시도합니다).\n",
        "\n",
        "훈련이 계속됨에 따라, 점진적으로 **엡실론 값을 줄입니다. 왜냐하면 우리는 점점 더 적은 탐색과 더 많은 활용이 필요하기 때문입니다.**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>"
      ],
      "metadata": {
        "id": "bABpZFSP9pra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
        "  # Randomly generate a number between 0 and 1\n",
        "  random_num =\n",
        "  # if random_num > greater than epsilon --> exploitation\n",
        "  if random_num > epsilon:\n",
        "    # Take the action with the highest value given a state\n",
        "    # np.argmax can be useful here\n",
        "    action =\n",
        "  # else --> exploration\n",
        "  else:\n",
        "    action = # Take a random action\n",
        "\n",
        "  return action"
      ],
      "metadata": {
        "id": "c26ECFZp9p_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "cWkUaIQB8O3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
        "  # 0과 1 사이의 임의의 숫자를 생성합니다.\n",
        "  random_num = random.uniform(0,1)\n",
        "  # random_num이 epsilon보다 크다면 --> 활용\n",
        "  if random_num > epsilon:\n",
        "    # 주어진 상태에서 가장 높은 값을 가진 행동을 취합니다.\n",
        "    # 여기서 np.argmax가 유용하게 사용될 수 있습니다.\n",
        "    action = greedy_policy(Qtable, state)\n",
        "  # 그렇지 않다면 --> 탐색\n",
        "  else:\n",
        "    action = env.action_space.sample()  # 환경의 행동 공간에서 임의의 행동을 샘플링합니다.\n",
        "\n",
        "  return action\n"
      ],
      "metadata": {
        "id": "vrlewVI-8B37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 코드는 엡실론 탐욕(Epsilon-Greedy) 정책을 구현한 것입니다. 이 정책은 강화 학습에서 에이전트가 환경을 탐색하는 방식을 결정하는 데 사용되며, 탐색(Exploration)과 활용(Exploitation) 사이의 균형을 맞추는 데 중요한 역할을 합니다. 코드의 주요 목적은 주어진 상태에서 에이전트가 어떤 행동을 취할지 결정하는 것입니다.\n",
        "\n",
        "1. **랜덤 숫자 생성**: 코드는 먼저 `random.uniform(0,1)`을 사용하여 0과 1 사이의 임의의 숫자를 생성합니다. 이 숫자는 이후에 탐색을 할지 활용을 할지를 결정하는 데 사용됩니다.\n",
        "\n",
        "2. **활용(Exploitation)**: 생성된 랜덤 숫자가 엡실론 값보다 큰 경우, 에이전트는 활용을 합니다. 즉, 현재 상태에 대해 Q-테이블에서 가장 높은 예상 보상을 가진 행동을 선택합니다. 이는 `greedy_policy` 함수를 호출하여 수행되며, `np.argmax(Qtable[state][:])`를 통해 구현됩니다. `np.argmax`는 주어진 상태에 대한 모든 가능한 행동들 중에서 가장 높은 Q값을 가진 행동을 찾아내는 함수입니다.\n",
        "\n",
        "3. **탐색(Exploration)**: 반면, 생성된 랜덤 숫자가 엡실론 값 이하인 경우, 에이전트는 탐색을 합니다. 즉, 가능한 행동 공간에서 임의의 행동을 선택합니다. 이는 `env.action_space.sample()`을 통해 수행됩니다. 이렇게 임의의 행동을 선택함으로써 에이전트는 새로운 상태를 경험하고 학습할 기회를 얻게 됩니다.\n",
        "\n",
        "**왜 이렇게 짜는가?**: 이 방식을 사용하는 이유는 강화 학습에서 에이전트가 환경을 너무 일찍 판단하고 최적의 해를 놓칠 수 있는 '지역 최적화(local optimum)' 문제를 방지하기 위해서입니다. 엡실론 탐욕 정책을 통해 에이전트는 충분한 탐색을 통해 환경에 대한 더 많은 정보를 수집하고, 장기적으로 더 나은 결정을 내릴 수 있습니다. 탐색과 활용 사이의 이 균형을 맞추는 것은 강화 학습에서 중요한 도전 과제 중 하나입니다."
      ],
      "metadata": {
        "id": "uCGLZAVg-ms9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 하이퍼파라미터 정의 ⚙️\n",
        "\n",
        "탐색과 관련된 하이퍼파라미터들은 가장 중요한 것들 중 일부입니다.\n",
        "\n",
        "- 우리는 에이전트가 좋은 가치 근사치를 학습하기 위해 **상태 공간의 충분한 부분을 탐색**하는지 확인해야 합니다. 이를 위해, 엡실론의 점진적 감소가 필요합니다.\n",
        "- 엡실론을 너무 빠르게 감소시키면(감소율이 너무 높으면), **에이전트가 막힐 위험이 있습니다**. 에이전트가 상태 공간의 충분한 부분을 탐색하지 못했기 때문에 문제를 해결할 수 없습니다."
      ],
      "metadata": {
        "id": "gIkwM-IJ-KoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 파라미터\n",
        "n_training_episodes = 10000  # 총 훈련 에피소드 수\n",
        "learning_rate = 0.7          # 학습률\n",
        "\n",
        "# 평가 파라미터\n",
        "n_eval_episodes = 100        # 총 테스트 에피소드 수\n",
        "\n",
        "# 환경 파라미터\n",
        "env_id = \"FrozenLake-v1\"     # 환경의 이름\n",
        "max_steps = 99               # 에피소드 당 최대 스텝 수\n",
        "gamma = 0.95                 # 할인율\n",
        "eval_seed = []               # 환경의 평가 시드\n",
        "\n",
        "# 탐색 파라미터\n",
        "max_epsilon = 1.0             # 시작 시 탐색 확률\n",
        "min_epsilon = 0.05            # 최소 탐색 확률\n",
        "decay_rate = 0.0005           # 탐색 확률의 지수적 감소율\n"
      ],
      "metadata": {
        "id": "YxjEInTx-p_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련 루프 메서드 생성\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
        "\n",
        "훈련 루프는 다음과 같이 진행됩니다:\n",
        "\n",
        "```\n",
        "총 훈련 에피소드에 대해 반복:\n",
        "\n",
        "엡실론 감소(우리는 점점 더 적은 탐색이 필요)\n",
        "환경 리셋\n",
        "\n",
        "  최대 타임스텝에 대해 반복:    \n",
        "    엡실론 탐욕 정책을 사용하여 행동 At를 선택\n",
        "    행동(a)을 취하고 결과 상태(s')와 보상(r)을 관찰\n",
        "    벨만 방정식을 사용하여 Q-값 Q(s,a) 업데이트: Q(s,a) + 학습률 [R(s,a) + 감마 * max Q(s',a') - Q(s,a)]\n",
        "    만약 완료되면, 에피소드를 마침\n",
        "    다음 상태는 새로운 상태임\n",
        "```"
      ],
      "metadata": {
        "id": "a1L7tB6y-pzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 이미지에 제시된 알고리즘은 Q-Learning, 특히 Sarsamax 버전의 강화 학습 알고리즘의 의사 코드(pseudocode)입니다. 여기에서 각 줄의 내용을 단계별로 정리하겠습니다:\n",
        "\n",
        "1. **입력**: 정책 π, 양의 정수 num_episodes, 작은 양의 실수 α, GLIE 시퀀스 {εi}\n",
        "2. **출력**: 가치 함수 Q (만약 num_episodes가 충분히 크다면 Q ≈ qπ)\n",
        "3. **초기화**: Q를 임의로 설정합니다. (예: 모든 s에 대해 Q(s,a) = 0 및 종료 상태의 Q(terminal-state, :) = 0)\n",
        "   - 이 단계에서, 모든 상태와 행동 쌍에 대한 Q-값을 0 또는 다른 임의의 값으로 설정합니다. 종료 상태에 대한 모든 행동은 Q-값이 0입니다.\n",
        "\n",
        "4. **에피소드 반복 시작**: 1부터 num_episodes까지 반복합니다.\n",
        "   - ε ← εi: 각 에피소드에 대한 ε 값을 업데이트합니다.\n",
        "   - S0 관찰: 초기 상태를 관찰합니다.\n",
        "   - t ← 0: 시간 스텝을 0으로 초기화합니다.\n",
        "\n",
        "5. **내부 반복 시작**:\n",
        "   - At를 Q에서 유도된 정책(예: ε-greedy)을 사용하여 선택합니다. 이는 에이전트가 현재 상태에 대한 최적의 행동을 선택하거나 ε 확률로 무작위 행동을 선택하는 단계입니다.\n",
        "   - 행동 At를 취하고 보상 Rt+1 및 다음 상태 St+1을 관찰합니다.\n",
        "   - Q-값 업데이트: Q(St, At) ← Q(St, At) + α(Rt+1 + γ maxa Q(St+1, a) - Q(St, At))\n",
        "     - 이 식은 벨만 최적 방정식을 사용하여 현재 상태-행동 쌍의 Q-값을 업데이트합니다. 여기서 α는 학습률, γ는 할인율입니다.\n",
        "\n",
        "6. **내부 반복 종료 조건**: 현재 상태 St가 종료 상태일 때까지 내부 반복을 계속합니다.\n",
        "\n",
        "7. **에피소드 반복 종료**: 모든 에피소드에 대한 반복이 끝나면 외부 반복을 종료합니다.\n",
        "\n",
        "8. **Q 반환**: 훈련이 끝난 후, 최종 Q-테이블을 반환합니다.\n",
        "\n",
        "이 의사 코드는 Q-Learning 알고리즘의 전체적인 흐름을 나타내며, 각 스텝마다 강화 학습 에이전트가 어떻게 환경과 상호작용하고, 경험을 통해 학습하는지를 보여줍니다."
      ],
      "metadata": {
        "id": "QSOUh1mS_go9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  for episode in tqdm(range(n_training_episodes)):\n",
        "    # 엡실론 값을 줄입니다 (왜냐하면 우리는 점점 더 적은 탐색을 필요로 합니다)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    # 환경을 리셋합니다\n",
        "    state, info = env.reset()\n",
        "    step = 0\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    # 반복\n",
        "    for step in range(max_steps):\n",
        "      # 엡실론 탐욕 정책을 사용하여 행동 At를 선택합니다\n",
        "      action =\n",
        "\n",
        "      # 행동 At를 취하고 Rt+1과 St+1을 관찰합니다\n",
        "      # 행동(a)를 취하고 결과 상태(s')와 보상(r)을 관찰합니다\n",
        "      new_state, reward, terminated, truncated, info =\n",
        "\n",
        "      # Q(s,a)를 업데이트합니다: Q(s,a) := Q(s,a) + 학습률 [R(s,a) + 감마 * max Q(s',a') - Q(s,a)]\n",
        "      Qtable[state][action] =\n",
        "\n",
        "      # 만약 종료되었거나 중단되었다면 에피소드를 마칩니다\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "\n",
        "      # 다음 상태는 새로운 상태입니다\n",
        "      state = new_state\n",
        "  return Qtable\n"
      ],
      "metadata": {
        "id": "EZcxjFtT_g8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "c44647cb-55fa-4990-aa44-da5448620402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-24-0362642e3fd1>, line 14)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-0362642e3fd1>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    action =\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "OW_1AUPe_oio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  for episode in tqdm(range(n_training_episodes)):\n",
        "    # 엡실론을 감소시킵니다 (우리는 점점 더 적은 탐색을 필요로 합니다)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    # 환경을 리셋합니다\n",
        "    state, info = env.reset()\n",
        "    step = 0\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    # 반복합니다\n",
        "    for step in range(max_steps):\n",
        "      # 엡실론 탐욕 정책을 사용하여 행동 At를 선택합니다\n",
        "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
        "\n",
        "      # 행동 At를 취하고 보상 Rt+1과 상태 St+1을 관찰합니다\n",
        "      new_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "      # Q(s,a)를 업데이트합니다: Q(s,a) := Q(s,a) + 학습률 * (보상 + 감마 * Qtable에서 new_state의 최대값 - Q(s,a))\n",
        "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
        "\n",
        "      # 만약 terminated 상태이거나 truncated 상태라면 에피소드를 종료합니다\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "\n",
        "      # 다음 상태는 새로운 상태입니다\n",
        "      state = new_state\n",
        "  return Qtable\n"
      ],
      "metadata": {
        "id": "QfrzDSsT_B3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Q-Learning agent 🏃"
      ],
      "metadata": {
        "id": "mn8Y63Q-AHGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b94bdbe3df874115998ec0e0df2543fe",
            "6f1c361dea5248e2a0daa94ae72694e4",
            "d809bf409f98481b88984f3f50182470",
            "205fedb73be7466a93a931d14d161399",
            "d247aa3aff0143f2b49cd22450fa889e",
            "2c89752b086443de8664a9954397ac3a",
            "3826ae4e979b4892a6065f6ae981b677",
            "53534c2c7d0e47959c496f00bde74b32",
            "6f049d049643434287dde6fa23373e5d",
            "222adc55450c4d02aa3735b3e2b6466b",
            "7c98a5c7edcc49d392425f4dce8ed4f0"
          ]
        },
        "id": "D8RJrGvxAIC-",
        "outputId": "385dd0f3-0fc3-4030-f210-329c3d5fd03f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b94bdbe3df874115998ec0e0df2543fe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's see what our Q-Learning table looks like now 👀\n"
      ],
      "metadata": {
        "id": "rKs22j29AbWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_frozenlake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrxJ2hT4AauR",
        "outputId": "83ad88e7-c6c8-4bb0-a965-f554d2c79f92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n",
              "       [0.73509189, 0.        , 0.81450625, 0.77378094],\n",
              "       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n",
              "       [0.81450625, 0.        , 0.77378094, 0.77378094],\n",
              "       [0.77378094, 0.81450625, 0.        , 0.73509189],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.9025    , 0.        , 0.81450625],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.81450625, 0.        , 0.857375  , 0.77378094],\n",
              "       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n",
              "       [0.857375  , 0.95      , 0.        , 0.857375  ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.9025    , 0.95      , 0.857375  ],\n",
              "       [0.9025    , 0.95      , 1.        , 0.9025    ],\n",
              "       [0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 평가 방법 📝\n",
        "\n",
        "- 우리는 Q-Learning 에이전트를 테스트하기 위해 사용할 평가 방법을 정의했습니다."
      ],
      "metadata": {
        "id": "QCfLbLCTAkHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
        "  \"\"\"\n",
        "  에이전트를 ``n_eval_episodes`` 에피소드 동안 평가하고 평균 보상과 보상의 표준 편차를 반환합니다.\n",
        "  :param env: 평가 환경\n",
        "  :param max_steps: 에피소드당 최대 스텝 수\n",
        "  :param n_eval_episodes: 에이전트를 평가할 에피소드 수\n",
        "  :param Q: Q-테이블\n",
        "  :param seed: 평가 시드 배열 (taxi-v3용)\n",
        "  \"\"\"\n",
        "  episode_rewards = []  # 각 에피소드의 총 보상을 저장할 리스트\n",
        "  for episode in tqdm(range(n_eval_episodes)):  # 평가할 에피소드 수만큼 반복\n",
        "    if seed:\n",
        "      state, info = env.reset(seed=seed[episode])  # 시드가 있다면 시드를 사용하여 환경 리셋\n",
        "    else:\n",
        "      state, info = env.reset()  # 시드가 없다면 일반적으로 환경 리셋\n",
        "    step = 0\n",
        "    truncated = False\n",
        "    terminated = False\n",
        "    total_rewards_ep = 0  # 현재 에피소드의 총 보상\n",
        "\n",
        "    for step in range(max_steps):  # 최대 스텝 수만큼 반복\n",
        "      # 현재 상태에서 미래 보상이 예상되는 최대값을 가지는 행동을 선택\n",
        "      action = greedy_policy(Q, state)\n",
        "      new_state, reward, terminated, truncated, info = env.step(action)\n",
        "      total_rewards_ep += reward  # 보상을 총 보상에 추가\n",
        "\n",
        "      if terminated or truncated:  # 에피소드가 종료되거나 중단되면 반복 중단\n",
        "        break\n",
        "      state = new_state  # 새로운 상태를 현재 상태로 업데이트\n",
        "    episode_rewards.append(total_rewards_ep)  # 현재 에피소드의 총 보상을 리스트에 추가\n",
        "  mean_reward = np.mean(episode_rewards)  # 평균 보상 계산\n",
        "  std_reward = np.std(episode_rewards)  # 보상의 표준 편차 계산\n",
        "\n",
        "  return mean_reward, std_reward  # 평균 보상과 표준 편차 반환\n"
      ],
      "metadata": {
        "id": "A0dZtPlzAiyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 `evaluate_agent` 함수는 Q-Learning 에이전트의 성능을 평가하는 과정입니다. 평가는 에이전트가 환경에서 어떻게 작동하는지, 즉 주어진 정책(Q-테이블)을 바탕으로 얼마나 좋은 보상을 얻는지를 측정합니다. 다음 단계에 따라 진행됩니다:\n",
        "\n",
        "1. **환경 설정**: 평가할 환경(`env`), 에피소드 당 최대 스텝 수(`max_steps`), 평가할 에피소드의 수(`n_eval_episodes`), Q-테이블(`Q`), 그리고 선택적으로 환경의 시드(`seed`)를 매개변수로 받습니다.\n",
        "\n",
        "2. **에피소드 반복**: `n_eval_episodes`의 수만큼 에피소드를 반복 실행합니다. 각 에피소드에 대해서:\n",
        "\n",
        "   - 환경은 `env.reset()`을 호출함으로써 초기 상태로 리셋됩니다. 만약 `seed` 배열이 제공되면, 이를 환경 리셋에 사용하여 결과의 일관성을 보장합니다.\n",
        "   \n",
        "   - 내부적으로, 에피소드의 각 스텝에 대해 반복을 수행합니다. `max_steps`는 에피소드가 최대로 진행할 수 있는 스텝 수를 정의합니다.\n",
        "\n",
        "3. **행동 선택**: 현재 상태에 기반하여 `greedy_policy` 함수를 사용해 탐욕적으로 행동을 선택합니다. 이는 Q-테이블에서 주어진 상태에 대한 최대 Q-값을 가진 행동을 의미합니다.\n",
        "\n",
        "4. **행동 실행 및 관찰**: 선택된 행동을 환경에 적용하고(`env.step(action)`), 새로운 상태(`new_state`), 보상(`reward`), 그리고 에피소드가 종료되었는지(`terminated`) 또는 중단되었는지(`truncated`)를 관찰합니다.\n",
        "\n",
        "5. **보상 누적**: 각 스텝에서 얻은 보상을 총 보상에 더합니다(`total_rewards_ep += reward`).\n",
        "\n",
        "6. **에피소드 종료 조건 검사**: 에피소드가 `terminated` 또는 `truncated` 상태에 도달하면 현재 에피소드를 종료하고 다음 에피소드로 넘어갑니다.\n",
        "\n",
        "7. **에피소드 보상 기록**: 각 에피소드가 끝날 때마다, 그 에피소드에서 얻은 총 보상을 `episode_rewards` 리스트에 추가합니다.\n",
        "\n",
        "8. **통계 계산**: 모든 에피소드의 평가가 끝나면, 총 보상 리스트를 사용하여 평균 보상(`mean_reward`)과 보상의 표준 편차(`std_reward`)를 계산합니다.\n",
        "\n",
        "9. **결과 반환**: 계산된 평균 보상과 표준 편차를 반환하여 에이전트의 성능을 나타냅니다.\n",
        "\n",
        "이 평가 메서드는 에이전트가 얼마나 잘 학습했는지를 객관적으로 측정하는 데 사용됩니다. 평균 보상이 높고 표준 편차가 낮을수록 에이전트는 환경에서 일관되게 좋은 성능을 내고 있다고 볼 수 있습니다."
      ],
      "metadata": {
        "id": "2cL3fGBUA8x5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning 에이전트 평가하기 📈\n",
        "\n",
        "- 일반적으로 평균 보상이 1.0이 되어야 합니다.\n",
        "- **환경은 상대적으로 쉽습니다**. 왜냐하면 상태 공간이 매우 작기 때문입니다(16개). 시도해볼 수 있는 것은 [미끄러운 버전으로 교체하는 것](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)인데, 이는 확률성을 도입하여 환경을 더 복잡하게 만듭니다."
      ],
      "metadata": {
        "id": "L0bOeBzCBA9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate our Agent\n",
        "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
        "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "8ada78b632e84d87ae43b2f76473fcad",
            "9ff42d4fefd74a9cb3149fee5b765207",
            "78bd099a19694296b7b658f68ae5b62b",
            "4e70251f0ed444e0a043732d078a188e",
            "2fa0154be64645d2ad7247341da41e92",
            "7478c74676b04e38b1c3937a551cb50e",
            "da5cde5eab684c1eb3d2cd762e811c95",
            "18db73cbc98a4454ac4be4b6f4ea7f38",
            "b4f7d198b9204883bc006f8ee42487ae",
            "51c31013e0f249c1879368a181f9cd2d",
            "61cc4ed3e49349ae82ab7b9ceaab48e9"
          ]
        },
        "id": "jF-xpBnjA1ka",
        "outputId": "a4d0b98f-67de-4493-b496-6050781f73ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ada78b632e84d87ae43b2f76473fcad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean_reward=1.00 +/- 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do not modify this code"
      ],
      "metadata": {
        "id": "Qf8TolLMAIzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, snapshot_download\n",
        "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
        "\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import json"
      ],
      "metadata": {
        "id": "GgE9r7YlBnl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def record_video(env, Qtable, out_directory, fps=1):\n",
        "  \"\"\"\n",
        "  Generate a replay video of the agent\n",
        "  :param env\n",
        "  :param Qtable: Qtable of our agent\n",
        "  :param out_directory\n",
        "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
        "  \"\"\"\n",
        "  images = []\n",
        "  terminated = False\n",
        "  truncated = False\n",
        "  state, info = env.reset(seed=random.randint(0,500))\n",
        "  img = env.render()\n",
        "  images.append(img)\n",
        "  while not terminated or truncated:\n",
        "    # Take the action (index) that have the maximum expected future reward given that state\n",
        "    action = np.argmax(Qtable[state][:])\n",
        "    state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic\n",
        "    img = env.render()\n",
        "    images.append(img)\n",
        "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
      ],
      "metadata": {
        "id": "YWS3m0tYBolE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85a47836-df19-49d0-bcd5-f9cac33c5bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def push_to_hub(\n",
        "    repo_id, model, env, video_fps=1, local_repo_path=\"hub\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n",
        "    This method does the complete pipeline:\n",
        "    - It evaluates the model\n",
        "    - It generates the model card\n",
        "    - It generates a replay video of the agent\n",
        "    - It pushes everything to the Hub\n",
        "\n",
        "    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n",
        "    :param env\n",
        "    :param video_fps: how many frame per seconds to record our video replay\n",
        "    (with taxi-v3 and frozenlake-v1 we use 1)\n",
        "    :param local_repo_path: where the local repository is\n",
        "    \"\"\"\n",
        "    _, repo_name = repo_id.split(\"/\")\n",
        "\n",
        "    eval_env = env\n",
        "    api = HfApi()\n",
        "\n",
        "    # Step 1: Create the repo\n",
        "    repo_url = api.create_repo(\n",
        "        repo_id=repo_id,\n",
        "        exist_ok=True,\n",
        "    )\n",
        "\n",
        "    # Step 2: Download files\n",
        "    repo_local_path = Path(snapshot_download(repo_id=repo_id))\n",
        "\n",
        "    # Step 3: Save the model\n",
        "    if env.spec.kwargs.get(\"map_name\"):\n",
        "        model[\"map_name\"] = env.spec.kwargs.get(\"map_name\")\n",
        "        if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
        "            model[\"slippery\"] = False\n",
        "\n",
        "    # Pickle the model\n",
        "    with open((repo_local_path) / \"q-learning.pkl\", \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    # Step 4: Evaluate the model and build JSON with evaluation metrics\n",
        "    mean_reward, std_reward = evaluate_agent(\n",
        "        eval_env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"]\n",
        "    )\n",
        "\n",
        "    evaluate_data = {\n",
        "        \"env_id\": model[\"env_id\"],\n",
        "        \"mean_reward\": mean_reward,\n",
        "        \"n_eval_episodes\": model[\"n_eval_episodes\"],\n",
        "        \"eval_datetime\": datetime.datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Write a JSON file called \"results.json\" that will contain the\n",
        "    # evaluation results\n",
        "    with open(repo_local_path / \"results.json\", \"w\") as outfile:\n",
        "        json.dump(evaluate_data, outfile)\n",
        "\n",
        "    # Step 5: Create the model card\n",
        "    env_name = model[\"env_id\"]\n",
        "    if env.spec.kwargs.get(\"map_name\"):\n",
        "        env_name += \"-\" + env.spec.kwargs.get(\"map_name\")\n",
        "\n",
        "    if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
        "        env_name += \"-\" + \"no_slippery\"\n",
        "\n",
        "    metadata = {}\n",
        "    metadata[\"tags\"] = [env_name, \"q-learning\", \"reinforcement-learning\", \"custom-implementation\"]\n",
        "\n",
        "    # Add metrics\n",
        "    eval = metadata_eval_result(\n",
        "        model_pretty_name=repo_name,\n",
        "        task_pretty_name=\"reinforcement-learning\",\n",
        "        task_id=\"reinforcement-learning\",\n",
        "        metrics_pretty_name=\"mean_reward\",\n",
        "        metrics_id=\"mean_reward\",\n",
        "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
        "        dataset_pretty_name=env_name,\n",
        "        dataset_id=env_name,\n",
        "    )\n",
        "\n",
        "    # Merges both dictionaries\n",
        "    metadata = {**metadata, **eval}\n",
        "\n",
        "    model_card = f\"\"\"\n",
        "  # **Q-Learning** Agent playing1 **{env_id}**\n",
        "  This is a trained model of a **Q-Learning** agent playing **{env_id}** .\n",
        "\n",
        "  ## Usage\n",
        "\n",
        "  ```python\n",
        "\n",
        "  model = load_from_hub(repo_id=\"{repo_id}\", filename=\"q-learning.pkl\")\n",
        "\n",
        "  # Don't forget to check if you need to add additional attributes (is_slippery=False etc)\n",
        "  env = gym.make(model[\"env_id\"])\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "    evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n",
        "\n",
        "    readme_path = repo_local_path / \"README.md\"\n",
        "    readme = \"\"\n",
        "    print(readme_path.exists())\n",
        "    if readme_path.exists():\n",
        "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
        "            readme = f.read()\n",
        "    else:\n",
        "        readme = model_card\n",
        "\n",
        "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(readme)\n",
        "\n",
        "    # Save our metrics to Readme metadata\n",
        "    metadata_save(readme_path, metadata)\n",
        "\n",
        "    # Step 6: Record a video\n",
        "    video_path = repo_local_path / \"replay.mp4\"\n",
        "    record_video(env, model[\"qtable\"], video_path, video_fps)\n",
        "\n",
        "    # Step 7. Push everything to the Hub\n",
        "    api.upload_folder(\n",
        "        repo_id=repo_id,\n",
        "        folder_path=repo_local_path,\n",
        "        path_in_repo=\".\",\n",
        "    )\n",
        "\n",
        "    print(\"Your model is pushed to the Hub. You can view your model here: \", repo_url)"
      ],
      "metadata": {
        "id": "rkYXB4XSBpZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "\n",
        "def record_video(env, Qtable, out_directory, fps=1):\n",
        "    images = []  # 이미지를 저장할 리스트\n",
        "    state = env.reset()  # 환경을 리셋하고 초기 상태를 얻음\n",
        "    img = env.render(mode='rgb_array')  # 현재 환경의 이미지를 얻음\n",
        "    images.append(img)\n",
        "    terminated = False\n",
        "    while not terminated:\n",
        "        action = np.argmax(Qtable[state])  # Q-테이블을 사용하여 최적의 행동을 선택\n",
        "        new_state, reward, terminated, truncated, info = env.step(action)\n",
        "        img = env.render(mode='rgb_array')  # 취한 행동 이후의 환경 이미지를 얻음\n",
        "        images.append(img)\n",
        "    imageio.mimsave(out_directory, [np.array(img) for img in images], fps=fps)  # 이미지 리스트를 비디오 파일로 저장\n",
        "\n",
        "# 환경과 Q-테이블 설정\n",
        "env = gym.make('FrozenLake-v1', is_slippery=False, new_step_api=True)  # 예시 환경\n",
        "Qtable = np.zeros((env.observation_space.n, env.action_space.n))  # 예시 Q-테이블, 실제 사용시 학습된 Q-테이블을 사용해야 함\n",
        "out_directory = './video.mp4'  # 비디오를 저장할 경로\n",
        "\n",
        "# 녹화 실행\n",
        "record_video(env, Qtable, out_directory, fps=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "zzx98wdiBqTJ",
        "outputId": "a9bc300f-69ef-4eb0-ce28-1e601676d619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-f976f7399c55>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# 녹화 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mrecord_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-f976f7399c55>\u001b[0m in \u001b[0;36mrecord_video\u001b[0;34m(env, Qtable, out_directory, fps)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Q-테이블을 사용하여 최적의 행동을 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 취한 행동 이후의 환경 이미지를 얻음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 이미지 리스트를 비디오 파일로 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             )\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m     ) -> Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    420\u001b[0m         \u001b[0;34m\"\"\"Renders the environment.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             )\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;34m\"set `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             )\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             )\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m     ) -> Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    420\u001b[0m         \u001b[0;34m\"\"\"Renders the environment.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             )\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_render_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             )\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/toy_text/frozen_lake.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/toy_text/frozen_lake.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"single_rgb_array\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_gui\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render_gui\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/toy_text/frozen_lake.py\u001b[0m in \u001b[0;36m_render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"single_rgb_array\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             return np.transpose(\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurfarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixels3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_surface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m             )\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf video_output"
      ],
      "metadata": {
        "id": "hYYFEmSqKfuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def epsilon_greedy_policy(Q, state, epsilon):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return random.randint(0, env.action_space.n - 1)\n",
        "    else:\n",
        "        return np.argmax(Q[state])\n",
        "\n",
        "def train_agent(env, max_steps, n_episodes, alpha, gamma, epsilon_start, epsilon_end, epsilon_decay, out_directory, record_every):\n",
        "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    epsilon = epsilon_start\n",
        "\n",
        "    for episode in tqdm(range(n_episodes)):\n",
        "        state = env.reset()\n",
        "        truncated = False\n",
        "        terminated = False\n",
        "\n",
        "        images = []\n",
        "        step = 0\n",
        "\n",
        "        while not (terminated or truncated) and step < max_steps:\n",
        "            action = epsilon_greedy_policy(Q, state, epsilon)\n",
        "            new_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            if terminated:\n",
        "                target = reward\n",
        "            else:\n",
        "                target = reward + gamma * np.max(Q[new_state])\n",
        "\n",
        "            Q[state, action] = Q[state, action] + alpha * (target - Q[state, action])\n",
        "\n",
        "            if episode % record_every == 0:\n",
        "                img = env.render(mode='rgb_array')\n",
        "                images.append(img)\n",
        "\n",
        "            state = new_state\n",
        "            step += 1\n",
        "\n",
        "        if episode % record_every == 0:\n",
        "            video_path = os.path.join(out_directory, f'episode_{episode:05d}.mp4')\n",
        "            imageio.mimsave(video_path, [np.array(img) for img in images], fps=30)\n",
        "\n",
        "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
        "\n",
        "    return Q\n",
        "\n",
        "def create_final_video(out_directory, final_video_path, fps=30):\n",
        "    video_paths = [os.path.join(out_directory, f) for f in os.listdir(out_directory) if f.endswith('.mp4')]\n",
        "    video_paths.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "\n",
        "    with imageio.get_writer(final_video_path, fps=fps) as writer:\n",
        "        for video_path in video_paths:\n",
        "            video = imageio.get_reader(video_path)\n",
        "            for frame in video:\n",
        "                writer.append_data(frame)\n",
        "\n",
        "    for video_path in video_paths:\n",
        "        os.remove(video_path)\n",
        "\n",
        "env = gym.make('FrozenLake-v1', is_slippery=False, new_step_api=True)\n",
        "max_steps = 100\n",
        "# 수정된 하이퍼파라미터\n",
        "n_episodes = 50000\n",
        "alpha = 0.05\n",
        "gamma = 0.99\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.01\n",
        "epsilon_decay = 0.999\n",
        "record_every = 100\n",
        "out_directory = './training_videos'\n",
        "os.makedirs(out_directory, exist_ok=True)\n",
        "\n",
        "Qtable = train_agent(env, max_steps, n_episodes, alpha, gamma, epsilon_start, epsilon_end, epsilon_decay, out_directory, record_every)\n",
        "\n",
        "final_video_path = './training_video.mp4'\n",
        "create_final_video(out_directory, final_video_path, fps=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrk8tBHqCwhf",
        "outputId": "e479432d-a529-4014-fce5-b7630d0b03fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:54<00:00, 913.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CPO9FkaRP4J2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i0x5ZvZ-ov99"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}